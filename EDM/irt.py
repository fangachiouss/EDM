# -*- coding: utf-8 -*-
"""IRT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmPJKXZPHmhL6Jyye63nBO4WI8I9iOQi
"""

import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from scipy import sparse
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score, accuracy_score, log_loss
from sklearn.linear_model import LogisticRegression
from scipy.sparse import load_npz, hstack, csr_matrix

def prepare_irt(df,min_interactions_per_user=10):
    remove_nan_skills = True
    df["timestamp"] = pd.to_datetime(df["start_time"])
    df["timestamp"] = df["start_time"]
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["timestamp"] = df["timestamp"] - df["timestamp"].min()
    df["timestamp"] = df["timestamp"].apply(lambda x: x.total_seconds()).astype(np.int64)
    df.sort_values(by="timestamp", inplace=True)
    df.reset_index(inplace=True, drop=True)
    df = df.groupby("user_id").filter(lambda x: len(x) >= min_interactions_per_user)
    if remove_nan_skills:
        df = df[~df["skill_id"].isnull()]
    else:
        df.ix[df["skill_id"].isnull(), "skill_id"] = -1
    df["user_id"] = np.unique(df["user_id"], return_inverse=True)[1]
    df["item_id"] = np.unique(df["problem_id"], return_inverse=True)[1]
    df["skill_id"] = np.unique(df["skill_id"], return_inverse=True)[1]
    Q_mat = np.zeros((len(df["item_id"].unique()), len(df["skill_id"].unique())))
    item_skill = np.array(df[["item_id", "skill_id"]])
    for i in range(len(item_skill)):
        Q_mat[item_skill[i,0],item_skill[i,1]] = 1
    df.reset_index(inplace=True, drop=True) # Add unique identifier of the row
    df["inter_id"] = df.index
    df = df[['user_id', 'item_id', 'timestamp', 'correct', "inter_id"]]
    df = df[df.correct.isin([0,1])] # Remove potential continuous outcomes
    df['correct'] = df['correct'].astype(np.int32)
    Q_mat=sparse.csr_matrix(Q_mat)
    return df , Q_mat

def apprentissage_irt(df,qmat):
    dict_q_mat = {i:set() for i in range(qmat.shape[0])}
    for elt in np.argwhere(qmat == 1):
        dict_q_mat[elt[0]].add(elt[1])
    X={}
    X['df'] = np.empty((0,5)) # Keep track of the original dataset
    for stud_id in df["user_id"].unique():
        df_stud = df[df["user_id"]==stud_id][["user_id", "item_id", "timestamp", "correct", "inter_id"]].copy()
        df_stud.sort_values(by="timestamp", inplace=True) # Sort values 
        df_stud = np.array(df_stud)
        X['df'] = np.vstack((X['df'], df_stud))
    onehot = OneHotEncoder()
    X['users'] = onehot.fit_transform(X["df"][:,0].reshape(-1,1))
    X['items'] = onehot.fit_transform(X["df"][:,1].reshape(-1,1))
    sparse_df = sparse.hstack([sparse.csr_matrix(X['df']),sparse.hstack([X['users'], X['items']])]).tocsr()
    X=csr_matrix(sparse_df)
    all_users = np.unique(X[:,0].toarray().flatten())
    y = X[:,3].toarray().flatten()
    kf = KFold(n_splits=5, shuffle=True)
    splits = kf.split(all_users)
    for run_id, (i_user_train, i_user_test) in enumerate(splits):
        users_train = all_users[i_user_train]
        users_test = all_users[i_user_test]
        X_train = X[np.where(np.isin(X[:,0].toarray().flatten(),users_train))]
        y_train = X_train[:,3].toarray().flatten()
        X_test = X[np.where(np.isin(X[:,0].toarray().flatten(),users_test))]
        y_test = X_test[:,3].toarray().flatten()
        print('fitting...')
        model = LogisticRegression(solver="liblinear", max_iter=400)
        model.fit(X_train[:,5:], y_train) # the 5 first columns are the non-sparse dataset
        y_pred_test = model.predict_proba(X_test[:,5:])[:, 1]
        ACC_log = accuracy_score(y_test, np.round(y_pred_test))
        print('ACC_log', ACC_log)
        AUC_log = roc_auc_score(y_test, y_pred_test)
        print('auc_log', AUC_log)
        NLL_log = log_loss(y_test, y_pred_test)
        print('nll_log', NLL_log)







