# -*- coding: utf-8 -*-
"""IRT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmPJKXZPHmhL6Jyye63nBO4WI8I9iOQi
"""

import pandas as pd
import numpy as np
from scipy import sparse


df = pd.read_csv("gdrive/MyDrive/2012-2013-data-with-predictions-4-final.csv")



min_interactions_per_user = 10
remove_nan_skills = True

df["timestamp"] = pd.to_datetime(df["start_time"])
df["timestamp"] = df["start_time"]
df["timestamp"] = pd.to_datetime(df["timestamp"])
df["timestamp"] = df["timestamp"] - df["timestamp"].min()
df["timestamp"] = df["timestamp"].apply(lambda x: x.total_seconds()).astype(np.int64)

"""L'encodage du column 'timestamp'"""

df.sort_values(by="timestamp", inplace=True)

"""on essaye d'ordonner le dataframe by:timestamp"""

df.head(5)

df.reset_index(inplace=True, drop=True)

df.head(5)

df = df.groupby("user_id").filter(lambda x: len(x) >= min_interactions_per_user)

df

if remove_nan_skills:
  df = df[~df["skill_id"].isnull()]
else:
  df.ix[df["skill_id"].isnull(), "skill_id"] = -1

"""df["user_id"] devient array qui contient les indices de chaque user_id dans np.unique(df['user_id'])"""

df["user_id"] = np.unique(df["user_id"], return_inverse=True)[1]
df["item_id"] = np.unique(df["problem_id"], return_inverse=True)[1]
df["skill_id"] = np.unique(df["skill_id"], return_inverse=True)[1]

df['user_id']

"""df["user_id"] devient array qui contient les indices de chaque user_id dans np.unique(df['user_id'])"""

# Build Q-matrix
Q_mat = np.zeros((len(df["item_id"].unique()), len(df["skill_id"].unique())))
item_skill = np.array(df[["item_id", "skill_id"]])
for i in range(len(item_skill)):
  Q_mat[item_skill[i,0],item_skill[i,1]] = 1

"""Q_mat est une matrice qui contient des 0 et des 1 
l'interpretation:1(x,y) x=indice de l'item dans la matrice d'unicité
                        y=l'indice du skill dans la matrice unicité 
"""

df.reset_index(inplace=True, drop=True) # Add unique identifier of the row
df["inter_id"] = df.index
df = df[['user_id', 'item_id', 'timestamp', 'correct', "inter_id"]]
df = df[df.correct.isin([0,1])] # Remove potential continuous outcomes
df['correct'] = df['correct'].astype(np.int32) # Cast outcome as int32

df.head(5)

"""Save the dataframe and the Q-matrix"""

# Save data
sparse.save_npz("q_mat.npz", sparse.csr_matrix(Q_mat))
df.to_csv("preprocessed_data.csv", sep="\t", index=False)

"""Copy the data to GDrive"""

!cp q_mat.npz "/gdrive/My Drive/Colab Notebooks/data/assistments12/"
!cp preprocessed_data.csv "/gdrive/My Drive/Colab Notebooks/data/assistments12/"

import pandas as pd
import numpy as np
from scipy import sparse
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('preprocessed_data.csv', sep="\t")
qmat = sparse.load_npz('q_mat.npz').toarray()

qmat

df

dict_q_mat = {i:set() for i in range(qmat.shape[0])}
for elt in np.argwhere(qmat == 1):
  dict_q_mat[elt[0]].add(elt[1])

dict_q_mat

X={}
X['df'] = np.empty((0,5)) # Keep track of the original dataset
for stud_id in df["user_id"].unique():
  df_stud = df[df["user_id"]==stud_id][["user_id", "item_id", "timestamp", "correct", "inter_id"]].copy()
  df_stud.sort_values(by="timestamp", inplace=True) # Sort values 
  df_stud = np.array(df_stud)
  X['df'] = np.vstack((X['df'], df_stud))

"""X est un dictionanire avec key='df' value='dataframe regrouper par le user_id'"""

X

onehot = OneHotEncoder()
X['users'] = onehot.fit_transform(X["df"][:,0].reshape(-1,1))
print("Users encoded.")
X['items'] = onehot.fit_transform(X["df"][:,1].reshape(-1,1))
print("Items encoded.")
sparse_df = sparse.hstack([sparse.csr_matrix(X['df']),sparse.hstack([X['users'], X['items']])]).tocsr()
"""on utilise one hot encoder pour encoder les deux colonnes user_id et item_id  

sparse_df : X['df'] + l'encodage du user_id et item_id
"""


sparse.save_npz('X-IRT.npz', sparse_df)

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score, accuracy_score, log_loss
from sklearn.linear_model import LogisticRegression
from scipy.sparse import load_npz, hstack, csr_matrix
import pandas as pd
import numpy as np

X = csr_matrix(load_npz('X-IRT.npz'))
all_users = np.unique(X[:,0].toarray().flatten())
y = X[:,3].toarray().flatten()
qmat = load_npz("q_mat.npz")

from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True)
splits = kf.split(all_users)

for run_id, (i_user_train, i_user_test) in enumerate(splits):
  users_train = all_users[i_user_train]
  users_test = all_users[i_user_test]
  
  X_train = X[np.where(np.isin(X[:,0].toarray().flatten(),users_train))]
  y_train = X_train[:,3].toarray().flatten()
  X_test = X[np.where(np.isin(X[:,0].toarray().flatten(),users_test))]
  y_test = X_test[:,3].toarray().flatten()
  print('fitting...')
  # model = LogisticRegression(solver="lbfgs", max_iter=400)
  model = LogisticRegression(solver="liblinear", max_iter=400)
  model.fit(X_train[:,5:], y_train) # the 5 first columns are the non-sparse dataset
  y_pred_test = model.predict_proba(X_test[:,5:])[:, 1]
  #y_pred_test_ran = clf.predict_proba(X_test[:,5:])[:, 1]
  ACC_log = accuracy_score(y_test, np.round(y_pred_test))
  #ACC_ran = accuracy_score(y_test, np.round(y_pred_test_ran))
  print('ACC_log', ACC_log)
  #print('ACC_ran', ACC_ran)
  AUC_log = roc_auc_score(y_test, y_pred_test)
  #AUC_ran = roc_auc_score(y_test, y_pred_test_ran)
  print('auc_log', AUC_log)
  #print('auc_ran', AUC_ran)
  NLL_log = log_loss(y_test, y_pred_test)
  #NLL_ran = log_loss(y_test, y_pred_test_ran)
  print('nll_log', NLL_log)
  #print('nll_ran', NLL_ran)









